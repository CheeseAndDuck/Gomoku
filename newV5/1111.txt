该版本是参考AlphaGo Zero,与original最大的区别在于将MCTS由“选择→扩展→模拟→回溯”改为“选择→扩展→评估→回溯”
并且还加入了累积折扣奖惩，并且使用一个α来调节价值网络和累积折扣之间的权重（即随着对弈局越来越多，价值主要是靠价值网络的输出） （这一点与OriginalV2的改进是一样子的）
* 累积折扣奖惩是考虑了：己方连子奖励、阻止对方连子奖励、被阻止连子惩罚、步数惩罚
* 累积折扣因子：0.95

MCTS的相关参数设置：
每次决策的模拟次数：800
UCB探索系数：2

该版本相比于original 以及 originaV2，运行速度以及训练效果就好很多了
