# Gomoku
Gomoku AI game models

这只是自己做题目的写的（初学者），如果大佬看到什么问题或是有什么建议，非常希望您能够告诉我，谢谢！

一共包含三个版本
original 和 originalV2是参考了AlphaGo，其中所采用的MCTS是“选择→扩展→模拟→回溯”
newV5是参考了AlphaGo Zero，其中所采用的MCTS是“选择→扩展→评估→回溯”

originalV2和newV5都是结合价值评估网络和累积折扣奖惩，
* 累积折扣奖惩是考虑了：己方连子奖励、阻止对方连子奖励、被阻止连子惩罚、步数惩罚
* 并且使用一个α来调节价值网络和累积折扣之间的权重（即在训练前期主要是靠累积折扣奖惩来评估局面价值的，后面随着对弈局越来越多，价值评估主要是靠价值网络的输出）
* 设置的折扣因子均为0.95,仅仅尝试了0.9和0.95，很显然0.95的效果更好

设置的MCTS的参数三个版本都是一样的：
factor=2，UCB探索因子
simulations=800， 每次决策的模拟次数
测试了simulations为500、650、800后发现800时效果时最好的，但是由于simulations变大运行时间就会变长，因此没有再提高simulations了
由于棋盘是15*15，还是比较复杂的，因此选择了factor为2


